\ Jasper,

I'm glad you like [the name "jiffy"].
The length of a jiffy varies from computer to computer.
According to the Windows API documentation, it is supposed
to be a constant amount of time on a given computer
from a single start-up until the corresponding shut-down.
Unfortunately, some computers built back when Windows XP was new
were not able to support the constant time feature.
My computer has 1315891 jiffies per second,
or a little under 760 nanoseconds per jiffy.

\ Ah! A variable unit of measure! How typical of Windows!
\ Makes me think there might be a market for my new invention:
\ Elastic Yardsticks!

My test suite reports its overall run-time in milliseconds.
The individual tests report their run-times
in either milliseconds, microseconds, or nanoseconds,
depending on which units the run-time can be expressed in.
This made it easy to search the output file for the long-running tests.
The slow tests end like " which I got in 15 ms." or " which is OK in 3 ms."
The fast tests end like " which I got in 8 탎." or " which is OK in 227 탎."
On my computer, every test takes at least 4 탎.

\ Makes me glad we included the  character in the Osmosian font.

I am surprised at how consistent individual tests' run-times are
from one run to another.  The overall run-time still bounces around,
but is consistent enough to reinforce Aunt Tilly's advice.
The precision timers are good enough that if I make a code change
whose purpose is to speed things up, but the timers say
it either slowed things down or made no difference,
I promptly undo the code change.

\ Sounds good. But how can the overall time bounce around if
\ the individual times are consistent? Surely the overall time is
\ never less than the sum of the individual times, so where is
\ the extra time going (or coming from)?

Based on your comments, I will make the compiler's timers
show the most accurate possible number of milliseconds,
rounded down to the nearest millisecond.

I will also make it easy to show the total number of microseconds.

\ Okay with me.

I have experimented with converting times to a format like
789 days 01:23:45 678 ms 901 탎 234 ns
but I am not happy with it.  For most purposes,
only the largest non-zero time range (such as ms or 탎)
is useful.

\ Seems to me that in measurements that involve DAYS
\ any additional precision below HOURS or maybe MINUTES are
\ superfluous (and inappropriate to the context).
\ For example:
\ "I've been waiting 10 days for my package to arrive,"
\ is a reasonable thing to say. On the other hand,
\ "I've been waiting 10 days, 3 hours, 14 minutes, 12 seconds,
\ 5 milliseconds, 2 microseconds, and 1 nanosecond for
\ my package to arrive," just sounds silly. Especially since you
\ can't say it fast enough for the statement to be accurate.

\ -- Gerry