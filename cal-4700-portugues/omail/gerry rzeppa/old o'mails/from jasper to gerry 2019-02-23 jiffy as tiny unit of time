Gerry,

I have a couple questions about timers.

Current versions of CAL do their timing using "ticks", which are
measured in milliseconds, but only occur every 15 or 16 milliseconds.

I have the 64-bit bigints working.  This lets CAL get
precise timing metrics from computers that support the 
QueryPerformanceCounter and QueryPerformanceFrequency API calls.
Internally, my draft version of CAL calls its precise time units
"jiffies".  For display and output purposes, it converts jiffies
to normal units of time, such as milliseconds or microseconds.
If jiffies are not available, or the client code wants to use ticks,
it uses ticks.

First question:  Does "jiffy" seem like a good name for the
precise time unit?  Do you have another suggestion instead?

Second question:  The compiler's current timer outputs
are only accurate to about 15 milliseconds.
They report that some compiler steps take no time at all,
even if the steps actually take a few milliseconds.
Do you ever want more precise timings?
If so, would you want them in milliseconds or microseconds?

-- Jasper